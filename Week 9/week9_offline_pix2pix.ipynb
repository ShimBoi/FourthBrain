{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"week9_offline_pix2pix.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K_biPSGapgl8"},"source":["## Week 9 offline assignment -- training pix2pix"]},{"cell_type":"markdown","metadata":{"id":"c5gpDFXFpoz2"},"source":["This week's assignment is very straightforward -- it's just a straight run through the TensorFlow pix2pix tutorial. We've added a couple sections for some free response from you, but otherwise it mostly follows the original tutorial verbatim. You won't need to write much, if any, code, but you will need to run the code that's here and comment on what it's doing. There are several markdown cells labeled FREE RESPONSE, where we'd like to you to do this."]},{"cell_type":"markdown","metadata":{"id":"8N45YC8LqfIV"},"source":["------"]},{"cell_type":"markdown","metadata":{"id":"kIVjv2tFqj8D"},"source":["...including the original copyright for the template notebook..."]},{"cell_type":"markdown","metadata":{"id":"v1CUZ0dkOo_F"},"source":["##### Copyright 2019 The TensorFlow Authors.\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:21.619411Z","iopub.status.busy":"2021-07-30T01:22:21.618530Z","iopub.status.idle":"2021-07-30T01:22:21.621915Z","shell.execute_reply":"2021-07-30T01:22:21.621272Z"},"id":"qmkj-80IHxnd"},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xnMOsbqHz61"},"source":["# pix2pix: Image-to-image translation with a conditional GAN"]},{"cell_type":"markdown","metadata":{"id":"ITZuApL56Mny"},"source":["This tutorial demonstrates how to build and train a conditional generative adversarial network (cGAN) called pix2pix that learns a mapping from input images to output images, as described in [Image-to-image translation with conditional adversarial networks](https://arxiv.org/abs/1611.07004) by Isola et al. (2017). pix2pix is not application specific—it can be applied to a wide range of tasks, including synthesizing photos from label maps, generating colorized photos from black and white images, turning Google Maps photos into aerial images, and even transforming sketches into photos.\n","\n","In this example, your network will generate images of building facades using the [CMP Facade Database](http://cmp.felk.cvut.cz/~tylecr1/facade/) provided by the [Center for Machine Perception](http://cmp.felk.cvut.cz/) at the [Czech Technical University in Prague](https://www.cvut.cz/). To keep it short, you will use a [preprocessed copy]((https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/)) of this dataset created by the pix2pix authors.\n","\n","In the pix2pix cGAN, you condition on input images and generate corresponding output images. cGANs were first proposed in [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) (Mirza and Osindero, 2014)\n","\n","The architecture of your network will contain:\n","\n","- A generator with a [U-Net]([U-Net](https://arxiv.org/abs/1505.04597))-based architecture.\n","- A discriminator represented by a convolutional PatchGAN classifier (proposed in the [pix2pix paper](https://arxiv.org/abs/1611.07004)).\n","\n","Note that each epoch can take around 15 seconds on a single V100 GPU.\n","\n","Below are some examples of the output generated by the pix2pix cGAN after training for 200 epochs on the facades dataset (80k steps).\n","\n","![sample output_1](https://www.tensorflow.org/images/gan/pix2pix_1.png)\n","![sample output_2](https://www.tensorflow.org/images/gan/pix2pix_2.png)"]},{"cell_type":"markdown","metadata":{"id":"e1_Y75QXJS6h"},"source":["## Import TensorFlow and other libraries"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:21.629671Z","iopub.status.busy":"2021-07-30T01:22:21.629060Z","iopub.status.idle":"2021-07-30T01:22:23.728175Z","shell.execute_reply":"2021-07-30T01:22:23.727628Z"},"id":"YfIk2es3hJEd"},"source":["import tensorflow as tf\n","\n","import os\n","import pathlib\n","import time\n","import datetime\n","\n","from matplotlib import pyplot as plt\n","from IPython import display"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iYn4MdZnKCey"},"source":["## Load the dataset\n","\n","Download the CMP Facade Database data (30MB). Additional datasets are available in the same format [here](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/). In Colab you can select other datasets from the drop-down menu. Note that some of the other datasets are significantly larger (`edges2handbags` is 8GB). "]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:23.732909Z","iopub.status.busy":"2021-07-30T01:22:23.732305Z","iopub.status.idle":"2021-07-30T01:22:23.734732Z","shell.execute_reply":"2021-07-30T01:22:23.734230Z"},"id":"qp6IAZvEShNf"},"source":["dataset_name = \"facades\"\n","\n","# from original notebook, you can try other datasets on your own. Please use facades for your submission\n","# dataset_name = \"facades\" #@param [\"cityscapes\", \"edges2handbags\", \"edges2shoes\", \"facades\", \"maps\", \"night2day\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:23.739998Z","iopub.status.busy":"2021-07-30T01:22:23.739390Z","iopub.status.idle":"2021-07-30T01:22:29.586593Z","shell.execute_reply":"2021-07-30T01:22:29.586970Z"},"id":"Kn-k8kTXuAlv"},"source":["_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz'\n","\n","path_to_zip = tf.keras.utils.get_file(\n","    fname=f\"{dataset_name}.tar.gz\",\n","    origin=_URL,\n","    extract=True)\n","\n","path_to_zip  = pathlib.Path(path_to_zip)\n","\n","PATH = path_to_zip.parent/dataset_name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ed-BdA8MyzqL"},"source":["print('The dataset is now stored here: ', PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gZP6KJT8yqVl"},"source":["### Optional:\n","The original version of this notebook stores data in the ``.keras`` default directory, checkpoints in ``./training_checkpoints`` and TensorBoard logs in `./logs`. If you're running in Colab that means it'll be in your Colab instance and won't persist if Colab times you out for inactivity. We're not asking you to train for super long, so this probably won't be a huge issue, especially if you have Colab Pro or Pro+. That said, it might just be a good idea to mount your Google Drive and store both data and the checkpoints there."]},{"cell_type":"markdown","metadata":{"id":"1fUzsnerj1P3"},"source":["Each original image is of size `256 x 512` containing two `256 x 256` images:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:30.870795Z","iopub.status.busy":"2021-07-30T01:22:30.870106Z","iopub.status.idle":"2021-07-30T01:22:30.874162Z","shell.execute_reply":"2021-07-30T01:22:30.874552Z"},"id":"XGY1kiptguTQ"},"source":["sample_image = tf.io.read_file(str(PATH / 'train/1.jpg'))\n","sample_image = tf.io.decode_jpeg(sample_image)\n","print(sample_image.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:30.894410Z","iopub.status.busy":"2021-07-30T01:22:30.889745Z","iopub.status.idle":"2021-07-30T01:22:31.034424Z","shell.execute_reply":"2021-07-30T01:22:31.034841Z"},"id":"vJ2sO8Izg7QV"},"source":["plt.figure()\n","plt.imshow(sample_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2A5SU-qxPAqd"},"source":["You need to separate real building facade images from the architecture label images—all of which will be of size `256 x 256`.\n","\n","Define a function that loads image files and outputs two image tensors:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:31.040422Z","iopub.status.busy":"2021-07-30T01:22:31.039873Z","iopub.status.idle":"2021-07-30T01:22:31.041710Z","shell.execute_reply":"2021-07-30T01:22:31.042082Z"},"id":"aO9ZAGH5K3SY"},"source":["def load(image_file):\n","  # Read and decode an image file to a uint8 tensor\n","  image = tf.io.read_file(image_file)\n","  image = tf.image.decode_jpeg(image)\n","\n","  # Split each image tensor into two tensors:\n","  # - one with a real building facade image\n","  # - one with an architecture label image \n","  w = tf.shape(image)[1]\n","  w = w // 2\n","  input_image = image[:, w:, :]\n","  real_image = image[:, :w, :]\n","\n","  # Convert both images to float32 tensors\n","  input_image = tf.cast(input_image, tf.float32)\n","  real_image = tf.cast(real_image, tf.float32)\n","\n","  return input_image, real_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5ByHTlfE06P"},"source":["Plot a sample of the input (architecture label image) and real (building facade photo) images:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:31.046292Z","iopub.status.busy":"2021-07-30T01:22:31.045728Z","iopub.status.idle":"2021-07-30T01:22:31.575342Z","shell.execute_reply":"2021-07-30T01:22:31.575711Z"},"id":"4OLHMpsQ5aOv"},"source":["inp, re = load(str(PATH / 'train/100.jpg'))\n","# Casting to int for matplotlib to display the images\n","plt.figure()\n","plt.imshow(inp / 255.0)\n","plt.figure()\n","plt.imshow(re / 255.0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVuZQTfI_c-s"},"source":["As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004), you need to apply random jittering and mirroring to preprocess the training set.\n","\n","Define several functions that:\n","\n","1. Resize each `256 x 256` image to a larger height and width—`286 x 286`.\n","2. Randomly crop it back to `256 x 256`.\n","3. Randomly flip the image horizontally i.e. left to right (random mirroring).\n","4. Normalize the images to the `[-1, 1]` range."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:31.579917Z","iopub.status.busy":"2021-07-30T01:22:31.579355Z","iopub.status.idle":"2021-07-30T01:22:31.581039Z","shell.execute_reply":"2021-07-30T01:22:31.581404Z"},"id":"2CbTEt448b4R"},"source":["# The facade training set consist of 400 images\n","BUFFER_SIZE = 400\n","# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n","BATCH_SIZE = 1\n","# Each image is 256x256 in size\n","IMG_WIDTH = 256\n","IMG_HEIGHT = 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:31.585924Z","iopub.status.busy":"2021-07-30T01:22:31.585365Z","iopub.status.idle":"2021-07-30T01:22:31.587313Z","shell.execute_reply":"2021-07-30T01:22:31.586904Z"},"id":"rwwYQpu9FzDu"},"source":["def resize(input_image, real_image, height, width):\n","  input_image = tf.image.resize(input_image, [height, width],\n","                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","  real_image = tf.image.resize(real_image, [height, width],\n","                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","\n","  return input_image, real_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:31.591434Z","iopub.status.busy":"2021-07-30T01:22:31.590831Z","iopub.status.idle":"2021-07-30T01:22:31.592531Z","shell.execute_reply":"2021-07-30T01:22:31.592874Z"},"id":"Yn3IwqhiIszt"},"source":["def random_crop(input_image, real_image):\n","  stacked_image = tf.stack([input_image, real_image], axis=0)\n","  cropped_image = tf.image.random_crop(\n","      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n","\n","  return cropped_image[0], cropped_image[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:31.596745Z","iopub.status.busy":"2021-07-30T01:22:31.596179Z","iopub.status.idle":"2021-07-30T01:22:31.598030Z","shell.execute_reply":"2021-07-30T01:22:31.597635Z"},"id":"muhR2cgbLKWW"},"source":["# Normalizing the images to [-1, 1]\n","def normalize(input_image, real_image):\n","  input_image = (input_image / 127.5) - 1\n","  real_image = (real_image / 127.5) - 1\n","\n","  return input_image, real_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:31.602685Z","iopub.status.busy":"2021-07-30T01:22:31.602126Z","iopub.status.idle":"2021-07-30T01:22:31.604111Z","shell.execute_reply":"2021-07-30T01:22:31.604425Z"},"id":"fVQOjcPVLrUc"},"source":["@tf.function()\n","def random_jitter(input_image, real_image):\n","  # Resizing to 286x286\n","  input_image, real_image = resize(input_image, real_image, 286, 286)\n","\n","  # Random cropping back to 256x256\n","  input_image, real_image = random_crop(input_image, real_image)\n","\n","  if tf.random.uniform(()) > 0.5:\n","    # Random mirroring\n","    input_image = tf.image.flip_left_right(input_image)\n","    real_image = tf.image.flip_left_right(real_image)\n","\n","  return input_image, real_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qvkV-IDw2pML"},"source":["<ins>FREE RESONPSE</ins>\n","\n","What is the purpose of `@tf.function` in the cell above? If running this function without `@tf.function` takes $x$ milliseconds on average, do you expect the running time with `@tf.function` to be $>x$  or  $<x$?\n","\n","[ The @tf.function's running time will be < x but not by too much, since the function itself does not require heavy computation like graphing ] \n"]},{"cell_type":"markdown","metadata":{"id":"wfAQbzy799UV"},"source":["You can inspect some of the preprocessed output:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:31.609535Z","iopub.status.busy":"2021-07-30T01:22:31.608988Z","iopub.status.idle":"2021-07-30T01:22:32.031208Z","shell.execute_reply":"2021-07-30T01:22:32.031648Z"},"id":"n0OGdi6D92kM"},"source":["plt.figure(figsize=(6, 6))\n","for i in range(4):\n","  rj_inp, rj_re = random_jitter(inp, re)\n","  plt.subplot(2, 2, i + 1)\n","  plt.imshow(rj_inp / 255.0)\n","  plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3E9LGq3WBmsh"},"source":["Having checked that the loading and preprocessing works, let's define a couple of helper functions that load and preprocess the training and test sets:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:32.036413Z","iopub.status.busy":"2021-07-30T01:22:32.035800Z","iopub.status.idle":"2021-07-30T01:22:32.038257Z","shell.execute_reply":"2021-07-30T01:22:32.037807Z"},"id":"tyaP4hLJ8b4W"},"source":["def load_image_train(image_file):\n","  input_image, real_image = load(image_file)\n","  input_image, real_image = random_jitter(input_image, real_image)\n","  input_image, real_image = normalize(input_image, real_image)\n","\n","  return input_image, real_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:32.042705Z","iopub.status.busy":"2021-07-30T01:22:32.042142Z","iopub.status.idle":"2021-07-30T01:22:32.043857Z","shell.execute_reply":"2021-07-30T01:22:32.044198Z"},"id":"VB3Z6D_zKSru"},"source":["def load_image_test(image_file):\n","  input_image, real_image = load(image_file)\n","  input_image, real_image = resize(input_image, real_image,\n","                                   IMG_HEIGHT, IMG_WIDTH)\n","  input_image, real_image = normalize(input_image, real_image)\n","\n","  return input_image, real_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIGN6ouoQxt3"},"source":["## Build an input pipeline with `tf.data`"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:32.049251Z","iopub.status.busy":"2021-07-30T01:22:32.048337Z","iopub.status.idle":"2021-07-30T01:22:32.217212Z","shell.execute_reply":"2021-07-30T01:22:32.216675Z"},"id":"SQHmYSmk8b4b"},"source":["train_dataset = tf.data.Dataset.list_files(str(PATH / 'train/*.jpg'))\n","train_dataset = train_dataset.map(load_image_train,\n","                                  num_parallel_calls=tf.data.AUTOTUNE)\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n","train_dataset = train_dataset.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:32.222619Z","iopub.status.busy":"2021-07-30T01:22:32.221959Z","iopub.status.idle":"2021-07-30T01:22:32.281448Z","shell.execute_reply":"2021-07-30T01:22:32.280960Z"},"id":"MS9J0yA58b4g"},"source":["try:\n","  test_dataset = tf.data.Dataset.list_files(str(PATH / 'test/*.jpg'))\n","except tf.errors.InvalidArgumentError:\n","  test_dataset = tf.data.Dataset.list_files(str(PATH / 'val/*.jpg'))\n","test_dataset = test_dataset.map(load_image_test)\n","test_dataset = test_dataset.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k_uXGBGm6G2u"},"source":["Note: you can read more about the TensorFlow Data API prefetching functionality (which is parameterized by `tf.data.AUTOTUNE` above), at [this link](https://www.tensorflow.org/guide/data_performance#prefetching)."]},{"cell_type":"markdown","metadata":{"id":"THY-sZMiQ4UV"},"source":["## Build the generator\n","\n","The generator of your pix2pix cGAN is a _modified_ [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder (downsampler) and decoder (upsampler). (You can find out more about it in the [Image segmentation](https://www.tensorflow.org/tutorials/images/segmentation) tutorial and on the [U-Net project website](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).)\n","\n","- Each block in the encoder is: Convolution -> Batch normalization -> Leaky ReLU\n","- Each block in the decoder is: Transposed convolution -> Batch normalization -> Dropout (applied to the first 3 blocks) -> ReLU\n","- There are skip connections between the encoder and decoder (as in the U-Net)."]},{"cell_type":"markdown","metadata":{"id":"xKUhMona6pMQ"},"source":["*Note* this isn't the exact U-Net architecture we used in week 7. That's totally okay, being a U-Net type neural network is more about the having skip connections and the encoder-decoder approach, not necesarrily specific kernel sizes or strides. The pix2pix authors just found that the U-Net architecture specified below worked better on their datasets."]},{"cell_type":"markdown","metadata":{"id":"4MQPuBCgtldI"},"source":["Define the downsampler (encoder):"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:32.285729Z","iopub.status.busy":"2021-07-30T01:22:32.285109Z","iopub.status.idle":"2021-07-30T01:22:32.287372Z","shell.execute_reply":"2021-07-30T01:22:32.286912Z"},"id":"tqqvWxlw8b4l"},"source":["OUTPUT_CHANNELS = 3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lwmiwrV47hy2"},"source":["This is a single \"layer\" that consists of the sequence Convolution, (optionally) batch normalization, and ReLU."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:32.292519Z","iopub.status.busy":"2021-07-30T01:22:32.291920Z","iopub.status.idle":"2021-07-30T01:22:32.294099Z","shell.execute_reply":"2021-07-30T01:22:32.293617Z"},"id":"3R09ATE_SH9P"},"source":["def downsample(filters, size, apply_batchnorm=True):\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","  result = tf.keras.Sequential()\n","  result.add(\n","      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n","                             kernel_initializer=initializer, use_bias=False))\n","\n","  if apply_batchnorm:\n","    result.add(tf.keras.layers.BatchNormalization())\n","\n","  result.add(tf.keras.layers.LeakyReLU())\n","\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:32.298062Z","iopub.status.busy":"2021-07-30T01:22:32.297462Z","iopub.status.idle":"2021-07-30T01:22:39.992665Z","shell.execute_reply":"2021-07-30T01:22:39.993011Z"},"id":"a6_uCZCppTh7"},"source":["down_model = downsample(3, 4)\n","down_result = down_model(tf.expand_dims(inp, 0))\n","print (down_result.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SvT9EkTM76iV"},"source":["^^ the 256 x 256 image is 128 x 128 (downsampled by a factor of 2) after a single `downsample` layer"]},{"cell_type":"markdown","metadata":{"id":"aFI_Pa52tjLl"},"source":["Define the upsampler (decoder): Each upsampling layer is Convolution, batch normalization, (optionally) Dropout."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:39.998731Z","iopub.status.busy":"2021-07-30T01:22:39.998192Z","iopub.status.idle":"2021-07-30T01:22:39.999726Z","shell.execute_reply":"2021-07-30T01:22:40.000176Z"},"id":"nhgDsHClSQzP"},"source":["def upsample(filters, size, apply_dropout=False):\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","  result = tf.keras.Sequential()\n","  result.add(\n","    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n","                                    padding='same',\n","                                    kernel_initializer=initializer,\n","                                    use_bias=False))\n","\n","  result.add(tf.keras.layers.BatchNormalization())\n","\n","  if apply_dropout:\n","      result.add(tf.keras.layers.Dropout(0.5))\n","\n","  result.add(tf.keras.layers.ReLU())\n","\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:40.004883Z","iopub.status.busy":"2021-07-30T01:22:40.004326Z","iopub.status.idle":"2021-07-30T01:22:40.730265Z","shell.execute_reply":"2021-07-30T01:22:40.730611Z"},"id":"mz-ahSdsq0Oc"},"source":["up_model = upsample(3, 4)\n","up_result = up_model(down_result)\n","print (up_result.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cH2fUhKq8fKn"},"source":["^^ The sequence of downsampling and then upsampling with the same filter sizes returns us to our original resolution"]},{"cell_type":"markdown","metadata":{"id":"ueEJyRVrtZ-p"},"source":["Define the GAN generator with the downsampler and the upsampler:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:40.816665Z","iopub.status.busy":"2021-07-30T01:22:40.815990Z","iopub.status.idle":"2021-07-30T01:22:40.824587Z","shell.execute_reply":"2021-07-30T01:22:40.824958Z"},"id":"lFPI4Nu-8b4q"},"source":["def Generator():\n","  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n","\n","  down_stack = [\n","    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n","    downsample(128, 4),  # (batch_size, 64, 64, 128)\n","    downsample(256, 4),  # (batch_size, 32, 32, 256)\n","    downsample(512, 4),  # (batch_size, 16, 16, 512)\n","    downsample(512, 4),  # (batch_size, 8, 8, 512)\n","    downsample(512, 4),  # (batch_size, 4, 4, 512)\n","    downsample(512, 4),  # (batch_size, 2, 2, 512)\n","    downsample(512, 4),  # (batch_size, 1, 1, 512)\n","  ]\n","\n","  up_stack = [\n","    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n","    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n","    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n","    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n","    upsample(256, 4),  # (batch_size, 32, 32, 512)\n","    upsample(128, 4),  # (batch_size, 64, 64, 256)\n","    upsample(64, 4),  # (batch_size, 128, 128, 128)\n","  ]\n","\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n","                                         strides=2,\n","                                         padding='same',\n","                                         kernel_initializer=initializer,\n","                                         activation='tanh')  # (batch_size, 256, 256, 3)\n","\n","  x = inputs\n","\n","  # Downsampling through the model\n","  skips = []\n","  for down in down_stack:\n","    x = down(x)\n","    skips.append(x)\n","\n","  skips = reversed(skips[:-1])\n","\n","  # Upsampling and establishing the skip connections\n","  for up, skip in zip(up_stack, skips):\n","    x = up(x)\n","    x = tf.keras.layers.Concatenate()([x, skip])\n","\n","  x = last(x)\n","\n","  return tf.keras.Model(inputs=inputs, outputs=x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bHpFAH3i82Z2"},"source":["^Try to follow what's being done here, it's pretty slick and compact the way they've written it. \n","\n","\"Why dropout in only the first 3 upsampling layers\", you ask? The most-likely answer is \"lots of trial and error,\" AKA \"Graduate Student Descent.\" In fact, contrary to other GAN models, pix2pix doesn't add noise to the input of the generator, the randomness of generator outputs comes from these dropout layers."]},{"cell_type":"markdown","metadata":{"id":"Z4PKwrcQFYvF"},"source":["Visualize the generator model architecture:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:40.831830Z","iopub.status.busy":"2021-07-30T01:22:40.831184Z","iopub.status.idle":"2021-07-30T01:22:41.571986Z","shell.execute_reply":"2021-07-30T01:22:41.572415Z"},"id":"dIbRPFzjmV85"},"source":["generator = Generator()\n","tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z8kbgTK8FcPo"},"source":["Test the generator:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:41.579502Z","iopub.status.busy":"2021-07-30T01:22:41.577471Z","iopub.status.idle":"2021-07-30T01:22:41.988732Z","shell.execute_reply":"2021-07-30T01:22:41.989136Z"},"id":"U1N1_obwtdQH"},"source":["gen_output = generator(inp[tf.newaxis, ...], training=False)\n","plt.imshow(gen_output[0, ...])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dpDPEQXIAiQO"},"source":["### Define the generator loss\n","\n","GANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n","\n","- The generator loss is a sigmoid cross-entropy loss of the generated images and an **array of ones**.\n","- The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.\n","- This allows the generated image to become structurally similar to the target image.\n","- The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper."]},{"cell_type":"markdown","metadata":{"id":"juceCAxHnquA"},"source":["<ins>FREE RESPONSE</ins>\n","In the function above, the variable `disc_generated_output` is meant to be the output of the discriminator when it is run on an image produced by the generator. Explain why it is compared to a tensor of 1's. Why not a tensor of 0's or some other number?\n","\n","[The generated images are being assumed as having the label of 1, or being the real image, and the discriminator has to find out that they are all not real since they are generated. Using 1s penalizes the discriminator for considering the generated image as being real.]"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:41.993547Z","iopub.status.busy":"2021-07-30T01:22:41.992967Z","iopub.status.idle":"2021-07-30T01:22:41.994710Z","shell.execute_reply":"2021-07-30T01:22:41.995088Z"},"id":"cyhxTuvJyIHV"},"source":["LAMBDA = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:41.999278Z","iopub.status.busy":"2021-07-30T01:22:41.998649Z","iopub.status.idle":"2021-07-30T01:22:42.000818Z","shell.execute_reply":"2021-07-30T01:22:42.000416Z"},"id":"Q1Xbz5OaLj5C"},"source":["loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.005406Z","iopub.status.busy":"2021-07-30T01:22:42.004777Z","iopub.status.idle":"2021-07-30T01:22:42.006828Z","shell.execute_reply":"2021-07-30T01:22:42.006408Z"},"id":"90BIcCKcDMxz"},"source":["def generator_loss(disc_generated_output, gen_output, target):\n","  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n","\n","  # Mean absolute error\n","  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","\n","  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n","\n","  return total_gen_loss, gan_loss, l1_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fSZbDgESHIV6"},"source":["The training procedure for the generator is as follows:"]},{"cell_type":"markdown","metadata":{"id":"TlB-XMY5Awj9"},"source":["![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n"]},{"cell_type":"markdown","metadata":{"id":"ZTKZfoaoEF22"},"source":["## Build the discriminator\n","\n","The discriminator in the pix2pix cGAN is a convolutional PatchGAN classifier—it tries to classify if each image _patch_ is real or not real, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n","\n","- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n","- The shape of the output after the last layer is `(batch_size, 30, 30, 1)`.\n","- Each of the `30 x 30` outputs classifies a `70 x 70` portion of the input image.\n","- The discriminator receives 2 inputs: \n","    - The input image and the target image, which it should classify as real.\n","    - The input image and the generated image (the output of the generator), which it should classify as fake.\n","    - Use `tf.concat([inp, tar], axis=-1)` to concatenate these 2 inputs together."]},{"cell_type":"markdown","metadata":{"id":"XIuTeGL5v45m"},"source":["Let's define the discriminator:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.014862Z","iopub.status.busy":"2021-07-30T01:22:42.014237Z","iopub.status.idle":"2021-07-30T01:22:42.016337Z","shell.execute_reply":"2021-07-30T01:22:42.015909Z"},"id":"ll6aNeQx8b4v"},"source":["def Discriminator():\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n","  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n","\n","  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n","\n","  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n","  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n","  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n","\n","  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n","  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n","                                kernel_initializer=initializer,\n","                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n","\n","  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n","\n","  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n","\n","  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n","\n","  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n","                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n","\n","  return tf.keras.Model(inputs=[inp, tar], outputs=last)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HdV9yAbBHNkg"},"source":["Visualize the discriminator model architecture:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.024060Z","iopub.status.busy":"2021-07-30T01:22:42.023489Z","iopub.status.idle":"2021-07-30T01:22:42.253383Z","shell.execute_reply":"2021-07-30T01:22:42.253773Z"},"id":"YHoUui4om-Ev"},"source":["discriminator = Discriminator()\n","tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ps7nIHigHYc7"},"source":["Test the discriminator:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.261244Z","iopub.status.busy":"2021-07-30T01:22:42.260668Z","iopub.status.idle":"2021-07-30T01:22:42.476375Z","shell.execute_reply":"2021-07-30T01:22:42.475830Z"},"id":"gDkA05NE6QMs"},"source":["disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\n","plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n","plt.colorbar()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VtaswXT7J5hB"},"source":["^These are the logits (un-normalized before a sigmoid) saying where it thinks the image is real (positive values) and where it thinks it's fake (negative values). This model hasn't been trained yet, which is why it might be doing a poor job identifying that `gen_output` is fake"]},{"cell_type":"markdown","metadata":{"id":"AOqg1dhUAWoD"},"source":["### Define the discriminator loss\n","\n","- The `discriminator_loss` function takes 2 inputs: **real images** and **generated images**.\n","- `real_loss` is a sigmoid cross-entropy loss of the **real images** and an **array of ones(since these are the real images)**.\n","- `generated_loss` is a sigmoid cross-entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**.\n","- The `total_loss` is the sum of `real_loss` and `generated_loss`."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.481159Z","iopub.status.busy":"2021-07-30T01:22:42.480584Z","iopub.status.idle":"2021-07-30T01:22:42.482657Z","shell.execute_reply":"2021-07-30T01:22:42.482133Z"},"id":"wkMNfBWlT-PV"},"source":["def discriminator_loss(disc_real_output, disc_generated_output):\n","  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)  # Binary cross entropy from logits\n","\n","  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n","\n","  total_disc_loss = real_loss + generated_loss\n","\n","  return total_disc_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ede4p2YELFa"},"source":["The training procedure for the discriminator is shown below.\n","\n","To learn more about the architecture and the hyperparameters you can refer to the [pix2pix paper](https://arxiv.org/abs/1611.07004)."]},{"cell_type":"markdown","metadata":{"id":"IS9sHa-1BoAF"},"source":["![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)\n"]},{"cell_type":"markdown","metadata":{"id":"0FMYgY_mPfTi"},"source":["## Define the optimizers and a checkpoint-saver\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.487724Z","iopub.status.busy":"2021-07-30T01:22:42.487144Z","iopub.status.idle":"2021-07-30T01:22:42.489175Z","shell.execute_reply":"2021-07-30T01:22:42.488733Z"},"id":"lbHFNexF0x6O"},"source":["generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.493943Z","iopub.status.busy":"2021-07-30T01:22:42.493385Z","iopub.status.idle":"2021-07-30T01:22:42.495331Z","shell.execute_reply":"2021-07-30T01:22:42.494900Z"},"id":"WJnftd5sQsv6"},"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                 discriminator_optimizer=discriminator_optimizer,\n","                                 generator=generator,\n","                                 discriminator=discriminator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zi0vFV1lKtma"},"source":["^ here is where you could point the script to a different checkpoint location, for instance if you have mounted your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"Rw1fkAczTQYh"},"source":["## Generate images\n","\n","Write a function to plot some images during training.\n","\n","- Pass images from the test set to the generator.\n","- The generator will then translate the input image into the output.\n","- The last step is to plot the predictions and _voila_!"]},{"cell_type":"markdown","metadata":{"id":"Rb0QQFHF-JfS"},"source":["Note: The `training=True` is intentional here since\n","you want the batch statistics, while running the model on the test dataset. If you use `training=False`, you get the accumulated statistics learned from the training dataset (which you don't want)."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.500867Z","iopub.status.busy":"2021-07-30T01:22:42.500262Z","iopub.status.idle":"2021-07-30T01:22:42.502448Z","shell.execute_reply":"2021-07-30T01:22:42.502008Z"},"id":"RmdVsmvhPxyy"},"source":["def generate_images(model, test_input, tar):\n","  prediction = model(test_input, training=True)\n","  plt.figure(figsize=(15, 15))\n","\n","  display_list = [test_input[0], tar[0], prediction[0]]\n","  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n","\n","  for i in range(3):\n","    plt.subplot(1, 3, i+1)\n","    plt.title(title[i])\n","    # Getting the pixel values in the [0, 1] range to plot.\n","    plt.imshow(display_list[i] * 0.5 + 0.5)\n","    plt.axis('off')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gipsSEoZIG1a"},"source":["Test the function:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.506650Z","iopub.status.busy":"2021-07-30T01:22:42.505992Z","iopub.status.idle":"2021-07-30T01:22:42.828953Z","shell.execute_reply":"2021-07-30T01:22:42.829317Z"},"id":"8Fc4NzT-DgEx"},"source":["for example_input, example_target in test_dataset.take(1):\n","  generate_images(generator, example_input, example_target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NLKOG55MErD0"},"source":["## Training\n","\n","- For each example input generates an output.\n","- The discriminator receives the `input_image` and the generated image as the first input. The second input is the `input_image` and the `target_image`.\n","- Next, calculate the generator and the discriminator loss.\n","- Then, calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n","- Finally, log the losses to TensorBoard."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.834737Z","iopub.status.busy":"2021-07-30T01:22:42.834118Z","iopub.status.idle":"2021-07-30T01:22:42.836327Z","shell.execute_reply":"2021-07-30T01:22:42.836701Z"},"id":"xNNMDBNH12q-"},"source":["log_dir=\"logs/\"\n","\n","summary_writer = tf.summary.create_file_writer(\n","  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.844768Z","iopub.status.busy":"2021-07-30T01:22:42.843935Z","iopub.status.idle":"2021-07-30T01:22:42.846387Z","shell.execute_reply":"2021-07-30T01:22:42.845927Z"},"id":"KBKUV2sKXDbY"},"source":["@tf.function\n","def train_step(input_image, target, step):\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","    gen_output = generator(input_image, training=True)\n","\n","    disc_real_output = discriminator([input_image, target], training=True)\n","    disc_generated_output = discriminator([input_image, gen_output], training=True)\n","\n","    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n","    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n","\n","  generator_gradients = gen_tape.gradient(gen_total_loss,\n","                                          generator.trainable_variables)\n","  discriminator_gradients = disc_tape.gradient(disc_loss,\n","                                               discriminator.trainable_variables)\n","\n","  generator_optimizer.apply_gradients(zip(generator_gradients,\n","                                          generator.trainable_variables))\n","  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n","                                              discriminator.trainable_variables))\n","\n","  with summary_writer.as_default():\n","    tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n","    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n","    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n","    tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hx7s-vBHFKdh"},"source":["The actual training loop. Since this tutorial can run of more than one dataset, and the datasets vary greatly in size the training loop is setup to work in steps instead of epochs.\n","\n","- Iterates over the number of steps.\n","- Every 10 steps print a dot (`.`).\n","- Every 1k steps: clear the display and run `generate_images` to show the progress. Also save a checkpoint"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.853250Z","iopub.status.busy":"2021-07-30T01:22:42.852384Z","iopub.status.idle":"2021-07-30T01:22:42.854775Z","shell.execute_reply":"2021-07-30T01:22:42.854346Z"},"id":"GFyPlBWv1B5j"},"source":["def fit(train_ds, test_ds, steps):\n","  example_input, example_target = next(iter(test_ds.take(1)))\n","  start = time.time()\n","\n","  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n","    if (step) % 1000 == 0:\n","      display.clear_output(wait=True)\n","\n","      if step != 0:\n","        print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n","\n","      start = time.time()\n","\n","      generate_images(generator, example_input, example_target)\n","      print(f\"Step: {step//1000}k\")\n","\n","    train_step(input_image, target, step)\n","\n","    # Training step\n","    if (step+1) % 10 == 0:\n","      print('.', end='', flush=True)\n","\n","\n","    # Save (checkpoint) the model every 5k steps\n","    if (step + 1) % 1000 == 0:\n","      checkpoint.save(file_prefix=checkpoint_prefix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wozqyTh2wmCu"},"source":["This training loop saves logs that you can view in TensorBoard to monitor the training progress.\n","\n","If you work on a local machine, you would launch a separate TensorBoard process. When working in a notebook, launch the viewer before starting the training to monitor with TensorBoard.\n","\n","To launch the viewer paste the following into a code-cell:"]},{"cell_type":"code","metadata":{"id":"Ot22ujrlLhOd"},"source":["%load_ext tensorboard\n","%tensorboard --logdir {log_dir}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pe0-8Bzg22ox"},"source":["Finally, run the training loop:"]},{"cell_type":"code","metadata":{"id":"WyEoqGC3NFOO"},"source":["# just checking what kind of GPU I was allocated...\n","!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NrhlBgwNL0lL"},"source":["FOURTHBRAIN_NUM_STEPS = 10000\n","# if you've got time an want to try to generate really good images, increase this as much as you want, the original tutorial used 40000.\n","# If your colab instance is timing out, it's okay to reduce this to several hundred, (but remember to change the checkpointing interval \n","# in the fit function above so that you actually have some checkpoints to choose from when you load in below)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:22:42.860231Z","iopub.status.busy":"2021-07-30T01:22:42.859339Z","iopub.status.idle":"2021-07-30T01:46:37.791493Z","shell.execute_reply":"2021-07-30T01:46:37.791988Z"},"id":"a1zZmKmvOH85"},"source":["fit(train_dataset, test_dataset, steps=FOURTHBRAIN_NUM_STEPS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DMTm4peo3cem"},"source":["Interpreting the logs is more subtle when training a GAN (or a cGAN like pix2pix) compared to a simple classification or regression model. Things to look for:\n","\n","- Check that neither the generator nor the discriminator model has \"won\". If either the `gen_gan_loss` or the `disc_loss` gets very low, it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n","- The value `log(2) = 0.69` is a good reference point for these losses, as it indicates a perplexity of 2 - the discriminator is, on average, equally uncertain about the two options.\n","- For the `disc_loss`, a value below `0.69` means the discriminator is doing better than random on the combined set of real and generated images.\n","- For the `gen_gan_loss`, a value below `0.69` means the generator is doing better than random at fooling the discriminator.\n","- As training progresses, the `gen_l1_loss` should go down."]},{"cell_type":"markdown","metadata":{"id":"S_8xYqppRM99"},"source":["^^ if you only trained for 10k steps like we recommended it may not have had time to settle down, but this is just a good rule of thumb for future reference"]},{"cell_type":"markdown","metadata":{"id":"kz80bY3aQ1VZ"},"source":["## Restore the latest checkpoint and test the network"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:46:37.805947Z","iopub.status.busy":"2021-07-30T01:46:37.805085Z","iopub.status.idle":"2021-07-30T01:46:37.994415Z","shell.execute_reply":"2021-07-30T01:46:37.993924Z"},"id":"HSSm4kfvJiqv"},"source":["!ls {checkpoint_dir}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:46:37.999988Z","iopub.status.busy":"2021-07-30T01:46:37.999042Z","iopub.status.idle":"2021-07-30T01:46:38.624374Z","shell.execute_reply":"2021-07-30T01:46:38.623835Z"},"id":"4t4x69adQ5xb"},"source":["# Restoring the latest checkpoint in checkpoint_dir\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1RGysMU_BZhx"},"source":["## Generate some images using the test set"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-30T01:46:38.629728Z","iopub.status.busy":"2021-07-30T01:46:38.628768Z","iopub.status.idle":"2021-07-30T01:46:40.303937Z","shell.execute_reply":"2021-07-30T01:46:40.304328Z"},"id":"KUgSnmy2nqSP"},"source":["# Run the trained model on a few examples from the test set\n","for inp, tar in test_dataset.take(5):\n","  generate_images(generator, inp, tar)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"10p58jkuRopI"},"source":["<ins>FREE RESPONSE</ins>\n","\n","What surprised you the most about the way that this conditional GAN is implemented in this notebook? Was there anything you found particularly confusing? What is the most interesting or useful application of GANs that you can find online with < 15 minutes of Googling around?\n","\n","[The generator's predictions based on the input images was extremely close to what the ground truth was. For example, it generated an image resembling an old church house, but even though the input had just the basic shapes and colored blocks, it was able to create an image that seemed like an old church. It had shading and faded coloring that you would expect. I'm still not quite sure how it can go from a 256x256 and go down to a 1x1 and still be able to go back to a 256x256 and retain most of the features/images in its prediction. The most interesting for me was text-to-image translation. Using GANs, it can create an image of whatever description a person puts in. It's crazy that ML models can understand the English language so well and be able to provide detailed images that we would expect only our brains to be able to do so well.]"]}]}